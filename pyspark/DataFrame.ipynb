{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.appName('test').master('local[*]').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.csv(\"测试数据/stu_score.txt\", sep=',', header=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, name: string, score: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df.toDF('id', 'name', 'score')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      "\n",
      "+---+----+-----+\n",
      "| id|name|score|\n",
      "+---+----+-----+\n",
      "|  1|语文|   99|\n",
      "|  2|语文|   99|\n",
      "|  3|语文|   99|\n",
      "|  4|语文|   99|\n",
      "|  5|语文|   99|\n",
      "|  6|语文|   99|\n",
      "|  7|语文|   99|\n",
      "|  8|语文|   99|\n",
      "|  9|语文|   99|\n",
      "| 10|语文|   99|\n",
      "| 11|语文|   99|\n",
      "| 12|语文|   99|\n",
      "| 13|语文|   99|\n",
      "| 14|语文|   99|\n",
      "| 15|语文|   99|\n",
      "| 16|语文|   99|\n",
      "| 17|语文|   99|\n",
      "| 18|语文|   99|\n",
      "| 19|语文|   99|\n",
      "| 20|语文|   99|\n",
      "+---+----+-----+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.printSchema(), df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "| id|name|score|\n",
      "+---+----+-----+\n",
      "|  1|语文|   99|\n",
      "|  2|语文|   99|\n",
      "|  3|语文|   99|\n",
      "|  4|语文|   99|\n",
      "|  5|语文|   99|\n",
      "+---+----+-----+\n"
     ]
    }
   ],
   "source": [
    "df2.createTempView(name='score') # 创建表 以SQL风格进行查询\n",
    "spark.sql(\"\"\"\n",
    "    select * from score where name='语文' limit 5\n",
    "\"\"\").show() # SQL风格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+\n",
      "| id|name|score|\n",
      "+---+----+-----+\n",
      "|  1|语文|   99|\n",
      "|  2|语文|   99|\n",
      "|  3|语文|   99|\n",
      "|  4|语文|   99|\n",
      "|  5|语文|   99|\n",
      "+---+----+-----+\n"
     ]
    }
   ],
   "source": [
    "df2.where(\"name='语文'\").limit(5).show() # 直接使用sparkAPI查询（DSL风格）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile('测试数据/sql/people.txt').map(lambda x: x.split(',')).map(lambda x: (x[0], int(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Michael', 29), ('Andy', 30), ('Justin', 19)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定要转换为DF的RDD； 指定列名，list形式给出\n",
    "df = spark.createDataFrame(rdd, schema=['name', 'age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "# 有RDD的数据类型推断出列类型 + 给定的列名 + 默认不为非空 得到schema信息\n",
    "df.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|name   |age|\n",
      "+-------+---+\n",
      "|Michael|29 |\n",
      "|Andy   |30 |\n",
      "|Justin |19 |\n",
      "+-------+---+\n"
     ]
    }
   ],
   "source": [
    "# 返回记录数，默认20； 是否对列的字符串长度超过20的截断 默认是True\n",
    "df.show(5, False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对DF对象创建临时视图表 供SQL查询\n",
    "df.createOrReplaceTempView(\"People\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "| Justin| 19|\n",
      "+-------+---+\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from People where age<30\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义StructType对象设置更加完整的schema\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "schema = StructType().add(\"name\", StringType(), nullable=False).add(\"age\", IntegerType(), nullable=False)\n",
    "df = spark.createDataFrame(rdd, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- age: integer (nullable = false)\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接使用rdd.toDF创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n"
     ]
    }
   ],
   "source": [
    "df = rdd.toDF(['name', 'age'])\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = false)\n",
      " |-- age: integer (nullable = false)\n",
      "\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|Michael| 29|\n",
      "|   Andy| 30|\n",
      "| Justin| 19|\n",
      "+-------+---+\n"
     ]
    }
   ],
   "source": [
    "df2 = rdd.toDF(schema=schema)\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Pandas的DF创建SparkSQL的DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n",
      "+---+----+---+\n",
      "| id|name|age|\n",
      "+---+----+---+\n",
      "|  1| zzj| 24|\n",
      "|  2| mmy| 24|\n",
      "|  3| ymm| 24|\n",
      "+---+----+---+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_pandas = pd.DataFrame({\n",
    "    \"id\": [1, 2, 3],\n",
    "    \"name\": ['zzj', 'mmy', 'ymm'],\n",
    "    'age': [24, 24, 24]\n",
    "})\n",
    "df = spark.createDataFrame(df_pandas)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不同数据源统一读取为DF的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: string (nullable = true)\n",
      "\n",
      "+-----------+\n",
      "|       data|\n",
      "+-----------+\n",
      "|Michael, 29|\n",
      "|   Andy, 30|\n",
      "| Justin, 19|\n",
      "+-----------+\n"
     ]
    }
   ],
   "source": [
    "# text数据源，只管把一整行作为字符串 列明默认为Value 设置了.option(\"sep\", ', ')也没用\n",
    "schema = StructType().add(\"data\", StringType(), nullable=False)\n",
    "df = spark.read.format(\"text\").schema(schema=schema).load(\"测试数据/sql/people.txt\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|NULL|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n"
     ]
    }
   ],
   "source": [
    "# 读取json数据源  自带schema 可以识别\n",
    "df = spark.read.format('json').load('测试数据/sql/people.json')\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      "\n",
      "+------------------+----+----+\n",
      "|              name| age| job|\n",
      "+------------------+----+----+\n",
      "|      name;age;job|NULL|NULL|\n",
      "|Jorge;30;Developer|NULL|NULL|\n",
      "|  Bob;32;Developer|NULL|NULL|\n",
      "|  Ani;11;Developer|NULL|NULL|\n",
      "|   Lily;11;Manager|NULL|NULL|\n",
      "|  Put;11;Developer|NULL|NULL|\n",
      "|   Alice;9;Manager|NULL|NULL|\n",
      "|   Alice;9;Manager|NULL|NULL|\n",
      "|   Alice;9;Manager|NULL|NULL|\n",
      "|   Alice;9;Manager|NULL|NULL|\n",
      "|    Alice;;Manager|NULL|NULL|\n",
      "|          Alice;9;|NULL|NULL|\n",
      "+------------------+----+----+\n"
     ]
    }
   ],
   "source": [
    "# 读取csv\n",
    "df = spark.read.format(\"csv\").option(\"sep\",',').option('header', False).option('encoding', 'utf-8')\\\n",
    "                             .schema(\"name STRING, age INT, job STRING\").load('测试数据/sql/people.csv')\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- favorite_color: string (nullable = true)\n",
      " |-- favorite_numbers: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      "\n",
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          NULL|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "# 读取parquet数据源 （Spark常用的列式存储文件格式 类似Hive的ORC） \n",
    "# 对比text 1.内置schema 2. 列式存储 3. 序列化存储（体积小） 直接load()读取为DF\n",
    "df = spark.read.format('parquet').load('测试数据/sql/users.parquet')\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSL风格的增删改查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").schema(\"id INT, subject STRING, score INT\").load('测试数据/stu_score.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "| id|subject|score|\n",
      "+---+-------+-----+\n",
      "|  1|   语文|   99|\n",
      "|  2|   语文|   99|\n",
      "|  3|   语文|   99|\n",
      "|  4|   语文|   99|\n",
      "|  5|   语文|   99|\n",
      "+---+-------+-----+\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|subject|\n",
      "+---+-------+\n",
      "|  1|   语文|\n",
      "|  2|   语文|\n",
      "|  3|   语文|\n",
      "|  4|   语文|\n",
      "|  5|   语文|\n",
      "+---+-------+\n"
     ]
    }
   ],
   "source": [
    "df.select(['id', 'subject']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|subject|\n",
      "+---+-------+\n",
      "|  1|   语文|\n",
      "|  2|   语文|\n",
      "|  3|   语文|\n",
      "|  4|   语文|\n",
      "|  5|   语文|\n",
      "+---+-------+\n"
     ]
    }
   ],
   "source": [
    "df.select(\"id\", \"subject\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Column<'id'>, Column<'subject'>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取Column对象 \n",
    "id_col = df['id']\n",
    "subject_col = df['subject']\n",
    "id_col, subject_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|subject|\n",
      "+---+-------+\n",
      "|  1|   语文|\n",
      "|  2|   语文|\n",
      "|  3|   语文|\n",
      "|  4|   语文|\n",
      "|  5|   语文|\n",
      "+---+-------+\n"
     ]
    }
   ],
   "source": [
    "df.select(id_col, subject_col).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "| id|subject|score|\n",
      "+---+-------+-----+\n",
      "|  1|   数学|   96|\n",
      "|  2|   数学|   96|\n",
      "|  3|   数学|   96|\n",
      "|  4|   数学|   96|\n",
      "|  5|   数学|   96|\n",
      "+---+-------+-----+\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"score < 99\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "| id|subject|score|\n",
      "+---+-------+-----+\n",
      "|  1|   数学|   96|\n",
      "|  2|   数学|   96|\n",
      "|  3|   数学|   96|\n",
      "|  4|   数学|   96|\n",
      "|  5|   数学|   96|\n",
      "+---+-------+-----+\n"
     ]
    }
   ],
   "source": [
    "df.where('score < 99').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-----+\n",
      "| id|subject|score|\n",
      "+---+-------+-----+\n",
      "|  1|   数学|   96|\n",
      "|  2|   数学|   96|\n",
      "|  3|   数学|   96|\n",
      "|  4|   数学|   96|\n",
      "|  5|   数学|   96|\n",
      "+---+-------+-----+\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['score'] < 99).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|subject|count|\n",
      "+-------+-----+\n",
      "|   英语|   30|\n",
      "|   语文|   30|\n",
      "|   数学|   30|\n",
      "+-------+-----+\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"subject\").count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|subject|count|\n",
      "+-------+-----+\n",
      "|   英语|   30|\n",
      "|   语文|   30|\n",
      "|   数学|   30|\n",
      "+-------+-----+\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df['subject']).count().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.group.GroupedData'>\n"
     ]
    }
   ],
   "source": [
    "# 不是DF对象，用来进一步聚合操作 聚合后才是DF对象可以show\n",
    "r = df.groupBy(\"subject\")\n",
    "print(type(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|subject|count|\n",
      "+-------+-----+\n",
      "|   英语|   30|\n",
      "|   语文|   30|\n",
      "|   数学|   30|\n",
      "+-------+-----+\n"
     ]
    }
   ],
   "source": [
    "r.count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL风格\n",
    "df.createTempView('score') # 注册临时视图\n",
    "df.createOrReplaceTempView('score2') # \n",
    "df.createGlobalTempView('score3') # 全局表查询时需要前缀global_temp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|subject|count(1)|\n",
      "+-------+--------+\n",
      "|   英语|      30|\n",
      "|   语文|      30|\n",
      "|   数学|      30|\n",
      "+-------+--------+\n",
      "\n",
      "+-------+--------+\n",
      "|subject|count(1)|\n",
      "+-------+--------+\n",
      "|   英语|      30|\n",
      "|   语文|      30|\n",
      "|   数学|      30|\n",
      "+-------+--------+\n",
      "\n",
      "+-------+--------+\n",
      "|subject|count(1)|\n",
      "+-------+--------+\n",
      "|   英语|      30|\n",
      "|   语文|      30|\n",
      "|   数学|      30|\n",
      "+-------+--------+\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select subject, count(*) from score group by subject\").show()\n",
    "spark.sql(\"select subject, count(*) from score2 group by subject\").show()\n",
    "spark.sql(\"select subject, count(*) from global_temp.score3 group by subject\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据处理API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+\n",
      "| name| age|      job|\n",
      "+-----+----+---------+\n",
      "|Jorge|  30|Developer|\n",
      "|  Bob|  32|Developer|\n",
      "|  Ani|  11|Developer|\n",
      "| Lily|  11|  Manager|\n",
      "|  Put|  11|Developer|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|NULL|  Manager|\n",
      "|Alice|   9|     NULL|\n",
      "+-----+----+---------+\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('csv').option('sep', ';').option('header', True).load('测试数据/sql/people.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+\n",
      "| name| age|      job|\n",
      "+-----+----+---------+\n",
      "|  Ani|  11|Developer|\n",
      "|Alice|   9|  Manager|\n",
      "| Lily|  11|  Manager|\n",
      "|  Put|  11|Developer|\n",
      "|Jorge|  30|Developer|\n",
      "|  Bob|  32|Developer|\n",
      "|Alice|   9|     NULL|\n",
      "|Alice|NULL|  Manager|\n",
      "+-----+----+---------+\n",
      "\n",
      "+-----+----+---------+\n",
      "| name| age|      job|\n",
      "+-----+----+---------+\n",
      "|Alice|NULL|  Manager|\n",
      "|  Ani|  11|Developer|\n",
      "| Lily|  11|  Manager|\n",
      "|Jorge|  30|Developer|\n",
      "|  Bob|  32|Developer|\n",
      "|Alice|   9|     NULL|\n",
      "|Alice|   9|  Manager|\n",
      "+-----+----+---------+\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates().show() # 删除整行重复的（每个列都要相同）\n",
    "df.dropDuplicates(['age', 'job']).show() # 指定不能重复的列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "|  Ani| 11|Developer|\n",
      "| Lily| 11|  Manager|\n",
      "|  Put| 11|Developer|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  9|  Manager|\n",
      "+-----+---+---------+\n",
      "\n",
      "+-----+----+---------+\n",
      "| name| age|      job|\n",
      "+-----+----+---------+\n",
      "|Jorge|  30|Developer|\n",
      "|  Bob|  32|Developer|\n",
      "|  Ani|  11|Developer|\n",
      "| Lily|  11|  Manager|\n",
      "|  Put|  11|Developer|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|NULL|  Manager|\n",
      "|Alice|   9|     NULL|\n",
      "+-----+----+---------+\n"
     ]
    }
   ],
   "source": [
    "# 缺失值处理\n",
    "df.dropna().show() # 有空值就删除该行\n",
    "df.dropna(thresh=1, subset=['age', 'job']).show() # subset中的有效列，必须有thresh个不为空，否则删除该行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "|  Ani| 11|Developer|\n",
      "| Lily| 11|  Manager|\n",
      "|  Put| 11|Developer|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|N/A|  Manager|\n",
      "|Alice|  9|      N/A|\n",
      "+-----+---+---------+\n",
      "\n",
      "+-----+----+---------+\n",
      "| name| age|      job|\n",
      "+-----+----+---------+\n",
      "|Jorge|  30|Developer|\n",
      "|  Bob|  32|Developer|\n",
      "|  Ani|  11|Developer|\n",
      "| Lily|  11|  Manager|\n",
      "|  Put|  11|Developer|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|   9|  Manager|\n",
      "|Alice|NULL|  Manager|\n",
      "|Alice|   9|      N/A|\n",
      "+-----+----+---------+\n",
      "\n",
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "|  Ani| 11|Developer|\n",
      "| Lily| 11|  Manager|\n",
      "|  Put| 11|Developer|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  9|  Manager|\n",
      "|Alice|  1|  Manager|\n",
      "|Alice|  9|   worker|\n",
      "+-----+---+---------+\n"
     ]
    }
   ],
   "source": [
    "# 填充缺失值\n",
    "df.fillna(value='N/A').show() # 全部缺失值都填为value\n",
    "df.fillna(value='N/A', subset=['job']).show() # 指定subset列补充缺值为value\n",
    "df.fillna({\"name\": '未知姓名', \"age\": 1, \"job\": \"worker\"}).show() # 都不同列 填指定的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写出API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "schema = StructType().add(\"user_id\", StringType(), nullable=True)\\\n",
    "                     .add(\"movie_id\", StringType(), nullable=True)\\\n",
    "                     .add(\"rank\", IntegerType(), nullable=True)\\\n",
    "                     .add(\"ts\", StringType(), nullable=True)\n",
    "df = spark.read.format(\"csv\").option(\"sep\", \"\\t\").schema(schema=schema).load('测试数据/sql/u.data')\n",
    "\n",
    "## 保存为text数据源，df只能是一列的 （读入也是df为一列）\n",
    "df.select(F.concat_ws('-', 'user_id', 'movie_id', 'rank', \"ts\")).write\\\n",
    "                            .mode(\"overwrite\").format('text')\\\n",
    "                            .option('sep', '-').option('header', True)\\\n",
    "                            .save('测试数据/save_df/text')\n",
    "\n",
    "## csv默认是根据','作为sep的\n",
    "df.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", ',').option('header', True).save(\"测试数据/save_df/csv\")\n",
    "\n",
    "## json不需要指定什么option\n",
    "df.write.format('json').mode('overwrite').save(\"测试数据/save_df/json\")\n",
    "\n",
    "## 默认是'parquent'格式数据源保存\n",
    "df.write.mode(\"overwrite\").save('测试数据/save_df/parquent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ame",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
